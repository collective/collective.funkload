.. -*-doctest-*-

================================================
collective.funkload
================================================
Complex functional load testing and benchmarking
------------------------------------------------

``collective.funkload`` provides some extensions of `Funkload
<http://pypi.python.org/pypi/funkload>`_, a web performance testing
and reporting tool.  These extensions provide flexible yet simple ways
to:

  - run benchmarks of multiple test scenarios
  - run these benchmarks against multiple setups
  - generate comparisons between those setups

The workflow collective.funkload is intended for is:

  1. develop test scenarios in Python eggs as with unittest

     Funkload test cases can be generated using the `funkload test
     recorder <>`_ and then placed in a Python egg's "tests" package
     as with normal `unittest <>`_ test cases.
     
     These test cases should be developed which reflect all of the
     application's supported usage patterns.  Take care to balance
     between separating tests by usage scenario (anonymous visitors,
     read access, write access, etc.) and keeping the number of tests
     low enough to scan results.

  2. benchmark the baseline setup

     Use the fl-run-bench provided by collective.funkload using
     `zope.testing.testrunner <>`_ semantics to specify the tests to
     run and the "--label" option to specify a label indicating the
     benchmark it the baseline::
     
       $ fl-run-bench -s foo.loadtests --label=baseline
       Running zope.testing.testrunner.layer.UnitTests tests:
         Set up zope.testing.testrunner.layer.UnitTests in 0.000 seconds.
       ========================================================================
       Benching FooTestCase.test_FooTest
       ========================================================================
       ...
       Bench status: **SUCCESS**
         Ran # tests with 0 failures and 0 errors in # minutes #.### seconds.
       Tearing down left over layers:
         Tear down zope.testing.testrunner.layer.UnitTests in 0.000 seconds.
     
     Use "fl-build-report --html" to build an HTML report from the XML
     generated by running the benchmark above.
     
       $ fl-build-report --html FooTest-bench-YYYYMMDDThhmmss.xml
       Creating html report ...done:
       file:///.../test_ReadOnly-YYYYMMDDThhmmss-baseline/index.html
     
     Examine the report details.  If the test cases don't sufficiently
     cover the application's supported useage patterns, repeat steps 1
     and 2 until the test cases provide sufficient coverage.

The scripts that Funkload installs generally require that they be
executed from the directory where the test modules live.  While this
is appropriate for generating test cases with the Funkload recorder,
it's often not the desirable behavior when recording benchmarking data
or generating reports.  Additionally, the argument handling for the
test and benchmark runners doesn't allow for specifying test modules
with dotted paths as one is often wont to do when working with
setuptools and eggs.

-------------------------
collective.funkload.bench
-------------------------

The collective.funkload package provides a wrapper around the Funkload
benchmark runner that handles dotted path arguments gracefully.
Specifically, rather than pass ``*.py`` file and TestCase.test_method
arguments, collective.funkload.bench.run() supports zope.testing
argument semantics for finding tests with "-s", "-m" and "-t".

    >>> from collective.funkload import bench
    >>> bench.run(defaults, (
    ...     'test.py -s foo -t test_foo '
    ...     '--cycles 1 --url http://bar.com').split())
    t...
    Benching FooTestCase.test_foo...
    * Server: http://bar.com...
    * Cycles: [1]...
